tar -zxvf zookeeper-3.4.13.tar.gz

cd zookeeper-3.4.13/conf

cp -p zoo_sample.cfg zoo.cfg

cd /opt/zookeeper/zookeeper-3.4.13/bin

./zkServer.sh start

cd /opt/kafka/

tar -zxvf kafka_2.12-2.1.0.tgz

cd /opt/kafka/kafka_2.12-2.1.0/bin

vim kafka-server-start.sh

    export KAFKA_HEAP_OPTS="-Xmx256m -Xms256m"

./kafka-server-start.sh -daemon ../config/server.properties

消息队列的核心:解耦、异步、削峰。
    1)解耦:A系统产生一条比较关键的数据, 很多系统都需要A系统将这个数据发送过来。(降低代码复杂度)
    2)异步:A系统接收一个请求, 需要在自己本地写库, 还需要在BCD三个系统写库。(减少请求耗时)
    3)削峰:每秒5k个请求写入MQ, A系统每秒钟最多处理2k个请求。(减轻系统压力, 保护系统, 一般mysql最大QPS为2000)

消息队列的缺点:
    1)系统可用性降低(mq挂了就全完了)
    2)系统复杂度提高(重复消费问题、消息丢失问题、消息顺序问题)
    3)分布式一致性问题

RabbitMQ、RocketMQ、Kafka:
    1)单机吞吐量:RabbitMQ万级。RocketMQ、Kafka10万级, 支撑高吞吐。
    2)时效性:RabbitMQ延迟最低。(微秒级, 其它是毫秒级)

申请kafka topic 可以设置partition数量、副本数量、数据保存时间。

Kafka的高可用性:
    1)Kafka的基本架构:由多个broker组成, 每个broker是一个节点。创建一个topic, 这个topic可以划分为多个partition, 每个partition可以存在于不同的broker上,
    每个partition只放一部分数据。这就是天然的分布式消息队列, 一个topic的数据, 是分散放在多个机器上的, 每个机器只放一部分数据。

    2)kafka副本机制:每个partition的数据都会同步到其它机器上, 形成自己的多个replica副本。
    所有replica会选举一个leader出来, 生产和消费都跟这个leader打交道, 然后其他replica就是follower。
    写的时候, leader会负责把数据同步到所有follower上去, 读的时候就直接读leader上的数据即可。(如果随意读写每个follower, 那么就要care数据一致性的问题, 系统复杂度太高, 很容易出问题)
    Kafka会均匀地将一个partition的所有replica分布在不同的机器上, 这样才可以提高容错性。

    3)如果某个broker宕机了, 这个broker上面的partition在其他机器上都有副本的, 如果这上面有某个partition的leader, 那么此时会从follower中重新选举一个新的leader出来。
    继续读写那个新的leader即可。

    4)写数据的时候, 生产者就写leader, 然后leader将数据落地写本地磁盘, 接着其他follower自己主动从leader来pull数据。一旦所有follower同步好数据了, 就会发送ack给leader。
    leader收到所有follower的ack之后, 就会返回写成功的消息给生产者。

    5)消费的时候, 只会从leader去读, 但是只有当一个消息已经被所有follower都同步成功返回ack的时候, 这个消息才会被消费者读到。

kafka offset:每个消息写进去, 都有一个offset, 代表消息的序号, 然后consumer消费了数据之后, 每隔一段时间(定时定期), 会把自己消费过的消息的offset提交一下。

Kafka重复消费问题:consumer有些消息处理完毕了, 但是没来得及提交offset, 此时系统被强制重启。

重复消费问题需要consumer保证幂等性:生产者发送每条数据的时候, 里面加一个全局唯一的id, 消费到了之后, 先根据这个id去redis里查一下, 之前消费过吗?
如果没有消费过, 就处理, 处理完毕后这个id写redis。如果消费过了, 就别处理了, 保证别重复处理相同的消息即可。

Kafka处理消息丢失:
    1)acks=all:这个是要求每条数据, 必须是写入所有replica之后, 才能认为是写成功了。(防止生产者弄丢数据)
    2)retries=MAX:要求一旦写入失败, 就无限重试, 卡在这里。
    3)消费端手动提交offset。(防止消费端弄丢了数据, 还没处理的消息被kafka自动提交了)
    4)防止kafka服务器丢失数据:
        *给topic设置replication.factor参数:这个值必须大于1, 要求每个partition必须有至少2个副本。
        *在Kafka服务端设置min.insync.replicas参数:这个值必须大于1, 这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系,
        这样才能确保leader挂了还有一个follower。(否则服务不可用)

Kafka保证消息顺序:将需要保证顺序的数据发送到相同partition, 然后使用多线程消费。(消费时, 把需要保持顺序的一组数据放在一个内存queue中, 使用一个线程单独处理这组数据)

zookeeper使用场景:
    1)分布式协调:A系统发送请求之后可以在zookeeper上对某个节点的值注册个监听器, 一旦B系统处理完了就修改zookeeper那个节点的值, A系统立马就可以收到通知。
    2)分布式锁。
    3)元数据/配置信息管理:dubbo、kafka等。
    4)HA高可用性:主备自动切换。(监听临时节点)

有一个redis cluster, 有5个redis master实例。然后执行如下步骤获取一把锁:
    1)轮流尝试在每个master节点上创建锁。

    2)尝试在大多数节点上建立一个锁, 比如5个节点就要求是3个节点(n / 2 + 1)
    客户端计算建立好锁的时间, 如果建立锁的时间小于超时时间, 就算建立成功了。

    3)要是锁建立失败了, 那么就依次之前建立过的锁删除。

    4)只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁。

同一个partition内的消息只能被同一个组中的一个consumer消费。当消费者数量多于partition的数量时, 多余的消费者空闲。
partition的数量决定了此topic在同一组中被可被均衡的程度, 例如partition = 4, 则可在同一组中被最多4个consumer均衡消费。

一旦consumer和kafka建立连接, consumer会以心跳的方式来通知kafka自己还活着, 如果session.timeout.ms内心跳未到达服务器, 服务器认为心跳丢失, 会做rebalance。

max.poll.interval.ms:用来标识消息处理的超时时间, 若超时, 会做rebalance。(两次poll()之间的最大时间间隔)

kafka写入磁盘的性能保障:一方面基于了os层面的page cache来写数据, 性能相当于操作缓存, 另外一方面, 他是采用磁盘顺序写的方式(数据追加到文件的末尾),
所以即使数据刷入磁盘的时候, 性能也是极高的, 也跟写内存是差不多的。

kafka从磁盘读取数据的性能保障:
    1)未优化的思路:先看看要读的数据在不在os cache里, 如果不在的话就从磁盘文件里读取数据后放入os cache。
    接着从操作系统的os cache里拷贝数据到应用程序进程的缓存里, 再从应用程序进程的缓存里拷贝数据到操作系统层面的Socket缓存里,
    最后从Socket缓存里提取数据后发送到网卡, 最后发送出去给下游消费。
    2)无用的拷贝:一次是从操作系统的cache里拷贝到应用进程的缓存里, 接着又从应用程序缓存里拷贝回操作系统的Socket缓存里。
    3)优化后:直接让操作系统cache中的数据发送到网卡后传输给下游的消费者, 中间跳过了两次拷贝数据的步骤。

kafka生产消息积压:
    1)修复consumer的问题, 并停止现有所有consumer。
    2)申请新的topic, partition数量为原来topic的10倍或20倍。
    3)将老的topic上的数据全部转发到新的topic上。
    4)部署原来10倍或20倍的consumer, 处理完数据后, 恢复之前消费者。
