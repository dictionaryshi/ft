tar -zxvf zookeeper-3.4.13.tar.gz

cd zookeeper-3.4.13/conf

cp -p zoo_sample.cfg zoo.cfg

cd /opt/zookeeper/zookeeper-3.4.13/bin

./zkServer.sh start

cd /opt/kafka/

tar -zxvf kafka_2.12-2.1.0.tgz

cd /opt/kafka/kafka_2.12-2.1.0/bin

vim kafka-server-start.sh

    export KAFKA_HEAP_OPTS="-Xmx256m -Xms256m"

./kafka-server-start.sh -daemon ../config/server.properties

消息队列的核心:解耦、异步、削峰。
    1)解耦:A系统产生一条比较关键的数据, 很多系统都需要A系统将这个数据发送过来。
    2)异步:A系统接收一个请求, 需要在自己本地写库, 还需要在BCD三个系统写库。
    3)削峰:每秒5k个请求写入MQ, A系统每秒钟最多处理2k个请求。

RabbitMQ、RocketMQ、Kafka:
    1)单机吞吐量:RabbitMQ万级。RocketMQ、Kafka10万级, 支撑高吞吐。
    2)topic数量对吞吐量的影响:RocketMQ在同等机器下, 可以支撑大量的topic。Kafka在同等机器下尽量保证topic数量不要过多。
    3)时效性:RabbitMQ延迟最低。
    4)可用性:Kafka一个数据多个副本, 少数机器宕机, 不会丢失数据, 不会导致不可用。
    5)消息可靠性:RocketMQ、Kafka可以做到0丢失。

Kafka高可用:由多个broker组成, 创建一个topic, 这个topic可以划分为多个partition,
每个partition可以存在于不同的broker上, 每个partition存放一部分数据。(一个topic数据分散放在多个机器上)

每个partition的数据都会同步到其它机器上, 形成自己的多个replica副本。所有replica会选举一个leader出来, 那么生产和消费都跟这个leader打交道。
然后其他replica就是follower。写的时候, leader会负责把数据同步到所有follower上去, 读的时候就直接读leader上的数据即可。
Kafka会均匀地将一个partition的所有replica分布在不同的机器上, 这样才可以提高容错性。

如果某个broker宕机了, 这个broker上面的partition在其他机器上都有副本的, 如果这上面有某个partition的leader, 那么此时会从follower中重新选举一个新的leader出来。
大家继续读写那个新的leader即可。

写数据的时候, 生产者就写leader, 然后leader将数据落地写本地磁盘, 接着其他follower自己主动从leader来pull数据。一旦所有follower同步好数据了, 就会发送ack给leader。
leader收到所有follower的ack之后, 就会返回写成功的消息给生产者。

消费的时候, 只会从leader去读, 但是只有当一个消息已经被所有follower都同步成功返回ack的时候, 这个消息才会被消费者读到。

Kafka重复消费问题:consumer有些消息处理了, 但是没来得及提交offset, 系统被强制重启。

Kafka处理消息丢失:
    1)acks=all:这个是要求每条数据, 必须是写入所有replica之后, 才能认为是写成功了。
    2)retries=MAX:要求一旦写入失败, 就无限重试, 卡在这里。
    3)消费端手动提交offset。

Kafka保证消息顺序:将需要保证顺序的数据发送到相同partition, 然后使用多线程消费。(消费时, 把需要保持顺序的一组数据放在一个内存queue中, 使用一个线程单独处理这组数据)

zookeeper使用场景:
    1)分布式协调:A系统发送请求之后可以在zookeeper上对某个节点的值注册个监听器, 一旦B系统处理完了就修改zookeeper那个节点的值, A系统立马就可以收到通知。
    2)分布式锁。
    3)元数据/配置信息管理:dubbo、kafka等。
    4)HA高可用性:主备自动切换。(监听临时节点)

有一个redis cluster, 有5个redis master实例。然后执行如下步骤获取一把锁:
    1)轮流尝试在每个master节点上创建锁。

    2)尝试在大多数节点上建立一个锁, 比如5个节点就要求是3个节点(n / 2 + 1)
    客户端计算建立好锁的时间, 如果建立锁的时间小于超时时间, 就算建立成功了。

    3)要是锁建立失败了, 那么就依次之前建立过的锁删除。

    4)只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁。

同一个partition内的消息只能被同一个组中的一个consumer消费。当消费者数量多于partition的数量时, 多余的消费者空闲。
也就是说如果只有一个partition你在同一组启动多少个consumer都没用, partition的数量决定了此topic在同一组中被可被均衡的程度, 例如partition = 4, 则可在同一组中被最多4个consumer均衡消费。

